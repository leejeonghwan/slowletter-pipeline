#!/usr/bin/env python3
"""
slowletter_pipeline.py
======================
ìŠ¬ë¡œìš°ë ˆí„° í¬ë¡¤ë§ + ì—”í‹°í‹° ì¶”ì¶œ í†µí•© íŒŒì´í”„ë¼ì¸

ì‚¬ìš©ë²•:
    python slowletter_pipeline.py                  # ì¦ë¶„ ì²˜ë¦¬ (ê¸°ë³¸)
    python slowletter_pipeline.py --mode full      # ì „ì²´ ì¬ì²˜ë¦¬
    python slowletter_pipeline.py --mode rebuild   # ì•„ì¹´ì´ë¸Œ ì´ˆê¸°í™” í›„ ì „ì²´ ì¬ìˆ˜ì§‘

í•„ìš”í•œ í™˜ê²½ë³€ìˆ˜:
    SOLAR_API_KEY   - Upstage Solar Pro2 API í‚¤

ë°ì´í„° ì €ì¥ ìœ„ì¹˜:
    ./data/slowletter_data_archives.csv          - í¬ë¡¤ë§ ì›ë³¸
    ./data/slowletter_entities.csv               - ì—”í‹°í‹° ì¶”ì¶œ ê²°ê³¼
    ./data/logs/                                 - ì‹¤í–‰ ë¡œê·¸
"""

import os
import re
import json
import time
import csv
import math
import threading
import argparse
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

import pandas as pd
import requests
from requests.adapters import HTTPAdapter, Retry
from bs4 import BeautifulSoup, NavigableString, Tag
from tqdm import tqdm

import warnings
warnings.filterwarnings("ignore")


# ============================================================
# ì„¤ì •
# ============================================================

# ê²½ë¡œ (í™˜ê²½ë³€ìˆ˜ë¡œ ë³€ê²½ ê°€ëŠ¥)
DATA_DIR = os.environ.get("DATA_DIR", os.path.join(os.path.dirname(__file__), "data"))
ARCHIVE_CSV = os.path.join(DATA_DIR, "slowletter_data_archives.csv")
ENTITIES_CSV = os.path.join(DATA_DIR, "slowletter_entities.csv")
LOG_DIR = os.path.join(DATA_DIR, "logs")

# í¬ë¡¤ë§ ì„¤ì •
WP_BASE_URL = "http://slownews.kr/wp-json/wp/v2/posts"
WP_CATEGORY_ID = 12637
WP_PER_PAGE = 100
WP_MAX_PAGES_FULL = 999
WP_MAX_PAGES_INCREMENTAL = 20
WP_SLEEP_SEC = 1.0

# ì—”í‹°í‹° ì¶”ì¶œ ì„¤ì •
SOLAR_BASE_URL = "https://api.upstage.ai/v1"
SOLAR_MODEL = "solar-pro2"
SOLAR_WORKERS = 2
SOLAR_QPS = 0.5
SOLAR_BATCH_SAVE = 500
SOLAR_TIMEOUT = 60
SOLAR_MAX_RETRIES = 10
SOLAR_BACKOFF = 5.0


# ============================================================
# ë¡œê¹… ì„¤ì •
# ============================================================

def setup_logging():
    os.makedirs(LOG_DIR, exist_ok=True)
    log_file = os.path.join(LOG_DIR, f"run_{datetime.now():%Y%m%d_%H%M%S}.log")

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[
            logging.FileHandler(log_file, encoding="utf-8"),
            logging.StreamHandler(),
        ],
    )
    return logging.getLogger(__name__)


# ============================================================
# STEP 1: í¬ë¡¤ë§
# ============================================================

def extract_li_content(li_tag) -> str:
    """li íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ì™€ a hrefë§Œ ì¶”ì¶œ."""
    parts = []
    for elem in li_tag.children:
        if isinstance(elem, NavigableString):
            text = str(elem).strip()
            if text:
                parts.append(text)
        elif elem.name == "a" and elem.get("href"):
            parts.append(f'<a href="{elem.get("href")}">{elem.get_text(strip=True)}</a>')
        elif elem.name in ("strong", "b", "em", "i", "mark", "span"):
            inner_parts = []
            for inner in elem.children:
                if isinstance(inner, NavigableString):
                    t = str(inner).strip()
                    if t:
                        inner_parts.append(t)
                elif inner.name == "a" and inner.get("href"):
                    inner_parts.append(
                        f'<a href="{inner.get("href")}">{inner.get_text(strip=True)}</a>'
                    )
                else:
                    t = inner.get_text(strip=True)
                    if t:
                        inner_parts.append(t)
            if inner_parts:
                parts.append("".join(inner_parts))
        else:
            t = elem.get_text(strip=True)
            if t:
                parts.append(t)
    return "".join(parts)


def load_archive(path: str) -> pd.DataFrame:
    """ê¸°ì¡´ ì•„ì¹´ì´ë¸Œ CSV ë¡œë“œ."""
    if not os.path.exists(path):
        return pd.DataFrame(
            columns=["uid", "post_id", "section_idx", "ID", "date", "title",
                      "h3_content", "api_id", "original_index"]
        )
    df = pd.read_csv(path, encoding="utf-8-sig")
    df.drop(columns=["big_section"], errors="ignore", inplace=True)
    if "date" in df.columns:
        df["date"] = pd.to_datetime(df["date"], format="mixed", errors="coerce")
    if "original_index" in df.columns:
        df["original_index"] = pd.to_numeric(df["original_index"], errors="coerce")
    return df


def migrate_legacy_archive(df: pd.DataFrame) -> pd.DataFrame:
    """êµ¬ë²„ì „ ì•„ì¹´ì´ë¸Œ(uid ì—†ìŒ)ë¥¼ ë§ˆì´ê·¸ë ˆì´ì…˜."""
    if df.empty:
        return df
    if "uid" in df.columns and df["uid"].notna().any():
        if "post_id" not in df.columns:
            df["post_id"] = df.get("api_id", pd.NA)
        if "section_idx" not in df.columns:
            df["section_idx"] = pd.NA
        return df

    if "api_id" not in df.columns:
        raise ValueError("legacy archiveì— api_idê°€ ì—†ì–´ ë§ˆì´ê·¸ë ˆì´ì…˜ ë¶ˆê°€")

    logging.getLogger(__name__).warning("legacy archive ê°ì§€ â†’ uid ìë™ ìƒì„±")
    sort_cols = [c for c in ["date", "api_id", "original_index", "title"] if c in df.columns]
    if sort_cols:
        df = df.sort_values(by=sort_cols).reset_index(drop=True)
    df["post_id"] = df["api_id"]
    df["section_idx"] = df.groupby("post_id").cumcount() + 1
    df["uid"] = df["post_id"].astype(str) + "_" + df["section_idx"].astype(int).astype(str).str.zfill(2)
    return df


def fetch_posts(category_id: int, since_date, mode: str, log) -> list:
    """WordPress REST APIì—ì„œ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘."""
    posts = []
    page = 1
    max_pages = WP_MAX_PAGES_FULL if mode == "full" else WP_MAX_PAGES_INCREMENTAL

    after_param = None
    if mode == "incremental" and since_date is not None and pd.notna(since_date):
        after_param = (since_date - pd.Timedelta(days=2)).to_pydatetime().isoformat()
        log.info(f"ì¦ë¶„ í¬ë¡¤ë§: {after_param} ì´í›„ í¬ìŠ¤íŠ¸ ìš”ì²­")

    while page <= max_pages:
        params = {
            "categories": category_id,
            "per_page": WP_PER_PAGE,
            "page": page,
            "orderby": "date",
            "order": "desc",
        }
        if after_param:
            params["after"] = after_param

        res = requests.get(WP_BASE_URL, params=params, timeout=30)
        data = res.json()

        if isinstance(data, dict):
            if data.get("code") == "rest_post_invalid_page_number":
                break
            raise RuntimeError(f"API ì—ëŸ¬ (page={page}): {data}")
        if not data:
            break

        posts.extend(data)
        log.info(f"  page {page}: +{len(data)} posts (ëˆ„ì  {len(posts)})")
        page += 1
        time.sleep(WP_SLEEP_SEC)

    return posts


def parse_h3_sections(posts: list) -> pd.DataFrame:
    """í¬ìŠ¤íŠ¸ HTMLì—ì„œ h3 ì„¹ì…˜ì„ íŒŒì‹±."""
    records = []
    for post in posts:
        post_id = post.get("id")
        date = post.get("date")
        html = (post.get("content") or {}).get("rendered", "") or ""
        soup = BeautifulSoup(html, "html.parser")

        section_idx = 0
        for h3 in soup.find_all("h3"):
            section_idx += 1
            h3_title = h3.get_text(" ", strip=True)
            content_parts = []
            sib = h3.find_next_sibling()
            while sib and getattr(sib, "name", None) not in ("h3", "h1", "div"):
                if getattr(sib, "name", None) in ("ul", "ol"):
                    for li in sib.find_all("li", recursive=False):
                        c = extract_li_content(li)
                        if c:
                            content_parts.append(f"<li>{c}</li>")
                sib = sib.find_next_sibling()

            records.append({
                "uid": f"{post_id}_{section_idx:02d}",
                "post_id": post_id,
                "section_idx": section_idx,
                "date": date,
                "title": h3_title,
                "h3_content": " ".join(content_parts),
                "api_id": post_id,
            })

    df = pd.DataFrame(records)
    if not df.empty:
        df["date"] = pd.to_datetime(df["date"], format="mixed", errors="coerce")
    return df


def merge_archive(archive_df: pd.DataFrame, new_df: pd.DataFrame) -> pd.DataFrame:
    """ì•„ì¹´ì´ë¸Œì™€ ìƒˆ ë°ì´í„°ë¥¼ ë³‘í•©, uid ê¸°ì¤€ ì¤‘ë³µ ì œê±°, ID ì¬ë¶€ì—¬."""
    if archive_df.empty:
        combined = new_df.copy()
    else:
        combined = pd.concat([archive_df, new_df], ignore_index=True)

    if combined.empty:
        return combined

    combined["date"] = pd.to_datetime(combined["date"], format="mixed", errors="coerce")
    combined["section_idx"] = pd.to_numeric(combined["section_idx"], errors="coerce")

    combined = combined.sort_values(by=["date", "uid"]).reset_index(drop=True)
    combined = combined.drop_duplicates(subset=["uid"], keep="last").reset_index(drop=True)

    combined = combined.sort_values(
        by=["date", "post_id", "section_idx"]
    ).reset_index(drop=True)
    combined["original_index"] = combined.index.astype(int)

    combined["date_str"] = combined["date"].dt.strftime("%Y_%m_%d")
    combined["date_rank"] = combined.groupby("date_str").cumcount() + 1
    combined["ID"] = combined["date_str"] + "_" + combined["date_rank"].astype(int).astype(str).str.zfill(2)
    combined.drop(columns=["date_str", "date_rank"], inplace=True)

    # ìµœì‹ ìˆœ ì •ë ¬
    combined = combined.sort_values(
        by=["date", "post_id", "section_idx"], ascending=[False, False, True]
    ).reset_index(drop=True)

    return combined


def step1_crawl(mode: str, log) -> pd.DataFrame:
    """Step 1: í¬ë¡¤ë§ ì‹¤í–‰."""
    log.info("=" * 50)
    log.info("STEP 1: í¬ë¡¤ë§ ì‹œì‘")
    log.info("=" * 50)

    os.makedirs(DATA_DIR, exist_ok=True)

    archive_df = load_archive(ARCHIVE_CSV)
    archive_df = migrate_legacy_archive(archive_df)
    log.info(f"ê¸°ì¡´ ì•„ì¹´ì´ë¸Œ: {len(archive_df)}ê±´")

    actual_mode = mode
    if mode == "auto":
        actual_mode = "incremental" if not archive_df.empty else "full"
    elif mode == "rebuild":
        log.info("rebuild ëª¨ë“œ: ì•„ì¹´ì´ë¸Œ ì´ˆê¸°í™”")
        archive_df = pd.DataFrame(columns=archive_df.columns)
        actual_mode = "full"

    latest_date = archive_df["date"].max() if not archive_df.empty else pd.NaT
    log.info(f"ëª¨ë“œ: {actual_mode} | ìµœì‹  ë‚ ì§œ: {latest_date}")

    posts = fetch_posts(WP_CATEGORY_ID, latest_date, actual_mode, log)
    log.info(f"ìˆ˜ì§‘í•œ í¬ìŠ¤íŠ¸: {len(posts)}ê°œ")

    h3_df = parse_h3_sections(posts)
    log.info(f"íŒŒì‹±í•œ h3 ì„¹ì…˜: {len(h3_df)}ê±´")

    archive_df = merge_archive(archive_df, h3_df)
    log.info(f"ë³‘í•© í›„ ì•„ì¹´ì´ë¸Œ: {len(archive_df)}ê±´")

    archive_df.to_csv(ARCHIVE_CSV, index=False, encoding="utf-8-sig")
    log.info(f"ì €ì¥ ì™„ë£Œ: {ARCHIVE_CSV}")

    return archive_df


# ============================================================
# STEP 2: ì—”í‹°í‹° ì¶”ì¶œ
# ============================================================

def clean_html_for_api(text: str) -> str:
    """HTML íƒœê·¸ ì œê±°, ê³µë°± ì •ê·œí™”."""
    if pd.isna(text) or text == "":
        return ""
    soup = BeautifulSoup(str(text), "html.parser")
    cleaned = soup.get_text().strip()
    return re.sub(r"\s+", " ", cleaned).strip()


def clean_html_for_service(text: str) -> str:
    """HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ, ì¤„ë°”ê¿ˆ ë³´ì¡´."""
    if pd.isna(text) or text == "":
        return ""
    soup = BeautifulSoup(str(text), "html.parser")
    parts = []
    for elem in soup.contents:
        if isinstance(elem, NavigableString):
            parts.append(str(elem).strip())
        elif isinstance(elem, Tag):
            if elem.name == "br":
                parts.append("\n")
            elif elem.name in ("li", "p", "div"):
                parts.append(elem.get_text(strip=True))
                parts.append("\n")
            else:
                parts.append(elem.get_text(strip=True))
    cleaned = "".join(parts).strip()
    cleaned = re.sub(r"\n\s*\n", "\n", cleaned)
    return re.sub(r"\s+", " ", cleaned).strip()


class QPSLimiter:
    """ì´ˆë‹¹ ìš”ì²­ ìˆ˜ ì œí•œ."""
    def __init__(self, qps: float):
        self.min_interval = 1.0 / max(qps, 0.01)
        self._lock = threading.Lock()
        self._last = 0.0

    def acquire(self):
        with self._lock:
            now = time.time()
            wait = self._last + self.min_interval - now
            if wait > 0:
                time.sleep(wait)
                now = time.time()
            self._last = now


class SolarEntityExtractor:
    """Upstage Solar Pro2 ì—”í‹°í‹° ì¶”ì¶œê¸°."""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.endpoint = f"{SOLAR_BASE_URL}/chat/completions"
        self.session = requests.Session()
        adapter = HTTPAdapter(
            max_retries=Retry(total=0),
            pool_connections=SOLAR_WORKERS * 2,
            pool_maxsize=SOLAR_WORKERS * 2,
        )
        self.session.mount("https://", adapter)
        self.session.headers.update({
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        })
        self.qps = QPSLimiter(SOLAR_QPS)
        self.total_requests = 0
        self.total_tokens = 0
        self.errors = 0

    def test_connection(self) -> bool:
        payload = {
            "model": SOLAR_MODEL,
            "messages": [{"role": "user", "content": "ì•ˆë…•"}],
            "max_tokens": 8,
        }
        try:
            self.qps.acquire()
            r = self.session.post(self.endpoint, json=payload, timeout=10)
            return r.status_code == 200
        except Exception:
            return False

    @staticmethod
    def _parse_json(content: str) -> Optional[Dict[str, List[str]]]:
        try:
            c = content.strip()
            if c.startswith("```json"):
                c = c[7:-3].strip()
            elif c.startswith("```"):
                c = c[3:-3].strip()
            data = json.loads(c)
            for k in ("persons", "organizations", "locations", "events", "concepts"):
                v = data.get(k, [])
                if not isinstance(v, list):
                    v = []
                data[k] = [str(x).strip() for x in v if isinstance(x, (str, int, float))][:10]
            return data
        except Exception:
            return None

    def extract(self, title: str, content: str) -> Dict[str, List[str]]:
        if len(content) > 1500:
            content = content[:1500] + "..."

        prompt = (
            "ë‹¤ìŒ í•œêµ­ì–´ ë‰´ìŠ¤ì—ì„œ í•µì‹¬ ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.\n\n"
            f"ì œëª©: {title}\n"
            f"ë‚´ìš©: {content}\n\n"
            "ë‹¤ìŒ 5ê°œ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì¤‘ìš”í•œ ìˆœì„œëŒ€ë¡œ ìµœëŒ€ 10ê°œì”© ì¶”ì¶œí•˜ì—¬ ì •í™•í•œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:\n\n"
            "1. persons: ì¸ë¬¼ëª… (ì •ì¹˜ì¸, ê¸°ì—…ì¸, ì—°ì˜ˆì¸, ì¼ë°˜ì¸ ë“±)\n"
            "2. organizations: ê¸°ê´€/ì¡°ì§ëª… (íšŒì‚¬, ì •ë¶€ê¸°ê´€, ì •ë‹¹, ë‹¨ì²´ ë“±)\n"
            "3. locations: ì§€ì—­ëª… (êµ­ê°€, ë„ì‹œ, êµ¬ì²´ì  ì¥ì†Œ ë“±)\n"
            "4. events: ì‚¬ê±´/ì‚¬ì•ˆëª… (êµ¬ì²´ì  ì‚¬ê±´, ì •ì±…, ì‚¬ê³  ë“±)\n"
            "5. concepts: í•µì‹¬ ê°œë…/í‚¤ì›Œë“œ (ê¸°ìˆ , ì‚¬íšŒí˜„ìƒ, ì •ì±… ë“±)\n\n"
            "ì‘ë‹µ í˜•ì‹ (ë°˜ë“œì‹œ ì´ í˜•ì‹ ì¤€ìˆ˜):\n"
            '{"persons": ["ì´ë¦„1", "ì´ë¦„2"], "organizations": ["ê¸°ê´€1"], '
            '"locations": ["ì§€ì—­1"], "events": ["ì‚¬ê±´1"], "concepts": ["ê°œë…1"]}\n\n'
            "ì£¼ì˜: ìœ íš¨í•œ JSONë§Œ ì¶œë ¥. ì„¤ëª…/ì£¼ì„ ì—†ì´. ì¤‘ë³µ ì œê±°. ë¹ˆ ë°°ì—´ë„ í¬í•¨."
        )

        payload = {
            "model": SOLAR_MODEL,
            "messages": [
                {"role": "system", "content": "ë‹¹ì‹ ì€ í•œêµ­ì–´ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìš”ì²­ë°›ì€ JSONìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."},
                {"role": "user", "content": prompt},
            ],
            "temperature": 0.1,
            "top_p": 0.9,
            "max_tokens": 600,
        }

        empty = {"persons": [], "organizations": [], "locations": [], "events": [], "concepts": []}

        for attempt in range(1, SOLAR_MAX_RETRIES + 1):
            try:
                self.qps.acquire()
                r = self.session.post(self.endpoint, json=payload, timeout=SOLAR_TIMEOUT)
                self.total_requests += 1

                if r.status_code == 200:
                    js = r.json()
                    usage = js.get("usage") or {}
                    self.total_tokens += int(usage.get("total_tokens", 0))
                    parsed = self._parse_json(js["choices"][0]["message"]["content"])
                    if parsed:
                        return parsed
                    time.sleep(min(3.0, SOLAR_BACKOFF * attempt * 0.5))
                    continue

                if r.status_code in (429, 500, 502, 503, 504):
                    wait = SOLAR_BACKOFF * (2 ** (attempt - 1))
                    time.sleep(wait)
                    continue

                self.errors += 1
                break

            except requests.exceptions.Timeout:
                time.sleep(SOLAR_BACKOFF * (2 ** (attempt - 1)))
            except (requests.exceptions.ConnectionError, requests.exceptions.ChunkedEncodingError):
                time.sleep(SOLAR_BACKOFF * (2 ** (attempt - 1)))
            except Exception:
                self.errors += 1
                break

        return empty


def load_existing_entities(path: str) -> pd.DataFrame:
    """ê¸°ì¡´ ì—”í‹°í‹° ê²°ê³¼ CSV ë¡œë“œ."""
    if not os.path.exists(path):
        return pd.DataFrame()
    df = pd.read_csv(path, encoding="utf-8")
    if "date" in df.columns:
        df["date"] = pd.to_datetime(df["date"], errors="coerce")
    return df


def step2_entities(archive_df: pd.DataFrame, log) -> pd.DataFrame:
    """Step 2: ì—”í‹°í‹° ì¶”ì¶œ."""
    log.info("=" * 50)
    log.info("STEP 2: ì—”í‹°í‹° ì¶”ì¶œ ì‹œì‘")
    log.info("=" * 50)

    api_key = os.environ.get("SOLAR_API_KEY", "")
    if not api_key:
        log.error("SOLAR_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        log.error("  export SOLAR_API_KEY='your-key-here'")
        return pd.DataFrame()

    extractor = SolarEntityExtractor(api_key)
    if not extractor.test_connection():
        log.error("Solar API ì—°ê²° ì‹¤íŒ¨. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return pd.DataFrame()
    log.info("Solar API ì—°ê²° ì„±ê³µ")

    # ì•„ì¹´ì´ë¸Œ ë°ì´í„° ì „ì²˜ë¦¬
    content_col = "h3_content" if "h3_content" in archive_df.columns else None
    if content_col is None:
        log.error("ì•„ì¹´ì´ë¸Œì— h3_content ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    archive_df["cleaned_content_for_api"] = archive_df[content_col].apply(clean_html_for_api)
    archive_df["cleaned_content_for_service"] = archive_df[content_col].apply(clean_html_for_service)

    # ê¸°ì¡´ ì—”í‹°í‹° ê²°ê³¼ ë¡œë“œ â†’ ì¦ë¶„ ì‹ë³„
    existing_df = load_existing_entities(ENTITIES_CSV)
    if existing_df.empty:
        to_process = archive_df.copy()
        log.info("ê¸°ì¡´ ì—”í‹°í‹° ê²°ê³¼ ì—†ìŒ â†’ ì „ì²´ ì²˜ë¦¬")
    else:
        existing_ids = set(existing_df["ID"].unique())
        archive_ids = set(archive_df["ID"].unique())
        new_ids = archive_ids - existing_ids
        if not new_ids:
            log.info("ìƒˆë¡œ ì²˜ë¦¬í•  ë°ì´í„° ì—†ìŒ")
            return existing_df
        to_process = archive_df[archive_df["ID"].isin(new_ids)].copy()
        log.info(f"ê¸°ì¡´: {len(existing_ids)}ê±´ | ì‹ ê·œ: {len(new_ids)}ê±´")

    log.info(f"ì—”í‹°í‹° ì¶”ì¶œ ëŒ€ìƒ: {len(to_process)}ê±´")

    # ë³‘ë ¬ ì¶”ì¶œ
    results = []
    results_lock = threading.Lock()

    def _work(row: dict) -> dict:
        try:
            entities = extractor.extract(
                row.get("title", ""),
                row.get("cleaned_content_for_api", ""),
            )
            return {
                "ID": row.get("ID"),
                "date": row.get("date"),
                "title": row.get("title", ""),
                "cleaned_content_for_api": row.get("cleaned_content_for_api", ""),
                "cleaned_content_for_service": row.get("cleaned_content_for_service", ""),
                "original_index": row.get("original_index"),
                "solar_persons": "; ".join(entities.get("persons", [])),
                "solar_organizations": "; ".join(entities.get("organizations", [])),
                "solar_locations": "; ".join(entities.get("locations", [])),
                "solar_events": "; ".join(entities.get("events", [])),
                "solar_concepts": "; ".join(entities.get("concepts", [])),
                "total_entities": sum(len(v) for v in entities.values()),
                "processed_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }
        except Exception as e:
            extractor.errors += 1
            return {
                "ID": row.get("ID"),
                "date": row.get("date"),
                "title": row.get("title", ""),
                "cleaned_content_for_api": row.get("cleaned_content_for_api", ""),
                "cleaned_content_for_service": row.get("cleaned_content_for_service", ""),
                "original_index": row.get("original_index"),
                "solar_persons": "", "solar_organizations": "",
                "solar_locations": "", "solar_events": "", "solar_concepts": "",
                "total_entities": 0,
                "processed_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "error": str(e),
            }

    with ThreadPoolExecutor(max_workers=SOLAR_WORKERS) as ex:
        futures = [ex.submit(_work, row.to_dict()) for _, row in to_process.iterrows()]
        for fut in tqdm(as_completed(futures), total=len(futures), desc="ì—”í‹°í‹° ì¶”ì¶œ"):
            results.append(fut.result())

    new_entities_df = pd.DataFrame(results)

    # ê¸°ì¡´ ê²°ê³¼ì™€ ë³‘í•©
    if not existing_df.empty:
        # ì»¬ëŸ¼ ë§ì¶”ê¸°
        all_cols = list(set(existing_df.columns) | set(new_entities_df.columns))
        for col in all_cols:
            if col not in existing_df.columns:
                existing_df[col] = None
            if col not in new_entities_df.columns:
                new_entities_df[col] = None
        final_df = pd.concat([existing_df, new_entities_df], ignore_index=True)
    else:
        final_df = new_entities_df

    if not final_df.empty:
        final_df["date"] = pd.to_datetime(final_df["date"], errors="coerce")
        final_df = final_df[final_df["date"].notna()].copy()
        final_df = final_df.sort_values("original_index").reset_index(drop=True)

    # ì €ì¥
    final_df.to_csv(ENTITIES_CSV, index=False, encoding="utf-8", quoting=csv.QUOTE_NONNUMERIC)
    log.info(f"ì—”í‹°í‹° ê²°ê³¼ ì €ì¥: {ENTITIES_CSV} ({len(final_df)}ê±´)")

    log.info(f"API ìš”ì²­: {extractor.total_requests} | í† í°: {extractor.total_tokens} | ì˜¤ë¥˜: {extractor.errors}")

    return final_df


# ============================================================
# ë©”ì¸
# ============================================================

def main():
    parser = argparse.ArgumentParser(description="ìŠ¬ë¡œìš°ë ˆí„° í¬ë¡¤ë§ + ì—”í‹°í‹° ì¶”ì¶œ íŒŒì´í”„ë¼ì¸")
    parser.add_argument(
        "--mode", choices=["auto", "incremental", "full", "rebuild"],
        default="auto",
        help="ì‹¤í–‰ ëª¨ë“œ (ê¸°ë³¸: auto = ì•„ì¹´ì´ë¸Œ ìˆìœ¼ë©´ ì¦ë¶„, ì—†ìœ¼ë©´ ì „ì²´)",
    )
    parser.add_argument(
        "--skip-crawl", action="store_true",
        help="í¬ë¡¤ë§ ê±´ë„ˆë›°ê¸° (ê¸°ì¡´ ì•„ì¹´ì´ë¸Œë¡œ ì—”í‹°í‹°ë§Œ ì¶”ì¶œ)",
    )
    parser.add_argument(
        "--skip-entity", action="store_true",
        help="ì—”í‹°í‹° ì¶”ì¶œ ê±´ë„ˆë›°ê¸° (í¬ë¡¤ë§ë§Œ ì‹¤í–‰)",
    )
    args = parser.parse_args()

    log = setup_logging()
    start_time = time.time()

    log.info("ğŸš€ ìŠ¬ë¡œìš°ë ˆí„° íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    log.info(f"   ëª¨ë“œ: {args.mode}")
    log.info(f"   ë°ì´í„° ê²½ë¡œ: {DATA_DIR}")

    # Step 1: í¬ë¡¤ë§
    if args.skip_crawl:
        log.info("í¬ë¡¤ë§ ê±´ë„ˆë›°ê¸° (--skip-crawl)")
        archive_df = load_archive(ARCHIVE_CSV)
        archive_df = migrate_legacy_archive(archive_df)
    else:
        archive_df = step1_crawl(args.mode, log)

    if archive_df.empty:
        log.warning("ì•„ì¹´ì´ë¸Œê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì¢…ë£Œ.")
        return

    # Step 2: ì—”í‹°í‹° ì¶”ì¶œ
    if args.skip_entity:
        log.info("ì—”í‹°í‹° ì¶”ì¶œ ê±´ë„ˆë›°ê¸° (--skip-entity)")
    else:
        step2_entities(archive_df, log)

    elapsed = time.time() - start_time
    log.info(f"âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (ì†Œìš”: {elapsed:.0f}ì´ˆ)")


if __name__ == "__main__":
    main()
