"""
cleaned_content_for_serviceì— HTML êµ¬ì¡°(ë¶ˆë ›, ë§í¬, ì¤„ë°”ê¿ˆ) ë³´ì¡´í•˜ê¸°.

ì‚¬ìš©ë²•:
  python update_service_content.py

- slowletter_data_archives.csv ì˜ h3_content (HTML ë³´ì¡´)ë¥¼
  slowletter_solar_entities.csv ì˜ cleaned_content_for_serviceì— ë°˜ì˜.
- cleaned_content_for_apiëŠ” ê·¸ëŒ€ë¡œ (í‰ë¬¸ ìœ ì§€).
- ì—”í‹°í‹° ì¬ì¶”ì¶œ ë¶ˆí•„ìš”.
"""

import pandas as pd
import re
import os

# --- ê²½ë¡œ ì„¤ì • (í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •) ---
ARCHIVE_CSV = os.path.expanduser("~/slowletter-pipeline/data/slowletter_data_archives.csv")
ENTITIES_CSV = os.path.expanduser("~/Downloads/work/data/raw/slowletter_solar_entities.csv")
OUTPUT_CSV = ENTITIES_CSV  # ë®ì–´ì“°ê¸° (ë°±ì—… ìë™ ìƒì„±)


def clean_html_for_service(h3_content: str) -> str:
    """
    h3_contentì—ì„œ <li>, <a href>, ì¤„ë°”ê¿ˆë§Œ ë³´ì¡´.
    bold, color, span ë“±ì€ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ë‚¨ê¹€.
    """
    if not h3_content or pd.isna(h3_content):
        return ""

    text = str(h3_content)

    # <a href="...">í…ìŠ¤íŠ¸</a> ë³´ì¡´
    # ë‚˜ë¨¸ì§€ íƒœê·¸ ì¤‘ <li>, </li>ë§Œ ë³´ì¡´

    # 1) <li>...</li> ê°ê°ì„ "â€¢ ë‚´ìš©\n" í˜•íƒœë¡œ ë³€í™˜
    #    li ì•ˆì˜ a íƒœê·¸ëŠ” ë³´ì¡´
    def process_li(match):
        inner = match.group(1)
        # a íƒœê·¸ ë³´ì¡´, ë‚˜ë¨¸ì§€ íƒœê·¸ ì œê±°
        inner = re.sub(r'<(?!/?a\b)[^>]+>', '', inner)
        inner = inner.strip()
        return f"â€¢ {inner}\n"

    text = re.sub(r'<li>(.*?)</li>', process_li, text, flags=re.DOTALL)

    # 2) ë‚¨ì€ íƒœê·¸ ì¤‘ a íƒœê·¸ ì™¸ ëª¨ë‘ ì œê±°
    text = re.sub(r'<(?!/?a\b)[^>]+>', '', text)

    # 3) ì—°ì† ê³µë°± ì •ë¦¬
    text = re.sub(r'[ \t]+', ' ', text)

    # 4) ì—°ì† ì¤„ë°”ê¿ˆ ì •ë¦¬
    text = re.sub(r'\n{3,}', '\n\n', text)

    return text.strip()


def main():
    print(f"ğŸ“‚ ì•„ì¹´ì´ë¸Œ CSV: {ARCHIVE_CSV}")
    print(f"ğŸ“‚ ì—”í‹°í‹° CSV: {ENTITIES_CSV}")

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(ARCHIVE_CSV):
        print(f"âŒ ì•„ì¹´ì´ë¸Œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {ARCHIVE_CSV}")
        return
    if not os.path.exists(ENTITIES_CSV):
        print(f"âŒ ì—”í‹°í‹° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {ENTITIES_CSV}")
        return

    # ë¡œë“œ
    print("ğŸ“– ì•„ì¹´ì´ë¸Œ CSV ë¡œë”©...")
    archive_df = pd.read_csv(ARCHIVE_CSV, encoding="utf-8-sig")
    print(f"   â†’ {len(archive_df)}ê±´")

    print("ğŸ“– ì—”í‹°í‹° CSV ë¡œë”©...")
    entities_df = pd.read_csv(ENTITIES_CSV)
    print(f"   â†’ {len(entities_df)}ê±´")

    # ID ê¸°ì¤€ h3_content ë§¤í•‘
    if "ID" not in archive_df.columns or "h3_content" not in archive_df.columns:
        print("âŒ ì•„ì¹´ì´ë¸Œì— ID ë˜ëŠ” h3_content ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    h3_map = dict(zip(archive_df["ID"], archive_df["h3_content"]))
    print(f"ğŸ”— ID ë§¤í•‘: {len(h3_map)}ê±´")

    # ë§¤ì¹­ í™•ì¸
    matched = entities_df["ID"].isin(h3_map).sum()
    print(f"âœ… ë§¤ì¹­: {matched}/{len(entities_df)}ê±´")

    # cleaned_content_for_service ì—…ë°ì´íŠ¸
    print("ğŸ”„ cleaned_content_for_service ì—…ë°ì´íŠ¸ ì¤‘...")
    entities_df["cleaned_content_for_service"] = entities_df["ID"].map(
        lambda x: clean_html_for_service(h3_map.get(x, ""))
    )

    # ë¹ˆ ê°’ ì²˜ë¦¬: ë§¤í•‘ ì•ˆ ëœ ê±´ ì›ë˜ ê°’ ìœ ì§€
    original = pd.read_csv(ENTITIES_CSV)
    mask = entities_df["cleaned_content_for_service"] == ""
    entities_df.loc[mask, "cleaned_content_for_service"] = original.loc[mask, "cleaned_content_for_service"]

    # ë°±ì—… ìƒì„±
    backup_path = ENTITIES_CSV + ".bak"
    os.rename(ENTITIES_CSV, backup_path)
    print(f"ğŸ’¾ ë°±ì—…: {backup_path}")

    # ì €ì¥
    entities_df.to_csv(OUTPUT_CSV, index=False)
    print(f"ğŸ’¾ ì €ì¥: {OUTPUT_CSV}")

    # ìƒ˜í”Œ í™•ì¸
    print("\n--- ìƒ˜í”Œ (ìµœì‹  3ê±´) ---")
    sample = entities_df.head(3)
    for _, row in sample.iterrows():
        print(f"\n[{row['ID']}] {row['title']}")
        content = row['cleaned_content_for_service']
        print(content[:300] if len(str(content)) > 300 else content)
        print("---")

    print("\nâœ… ì™„ë£Œ!")


if __name__ == "__main__":
    main()
